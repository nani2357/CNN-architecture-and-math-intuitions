{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    ">AlexNet was designed by Hinton, winner of the 2012 ImageNet competition, and his student Alex Krizhevsky. It was also after that year that more and deeper neural networks were proposed, such as the excellent vgg, GoogleLeNet. Its official data model has an accuracy rate of 57.1% and top 1-5 reaches 80.2%. This is already quite outstanding for traditional machine learning classification algorithms.\n",
    "\n",
    "\n",
    "![title](img/alexnet.png)\n",
    "\n",
    "\n",
    "![title](img/alexnet2.png)\n",
    "\n",
    ">The following table below explains the network structure of AlexNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<thead>\n",
    "\t<tr>\n",
    "\t\t<th>Size / Operation</th>\n",
    "\t\t<th>Filter</th>\n",
    "\t\t<th>Depth</th>\n",
    "\t\t<th>Stride</th>\n",
    "\t\t<th>Padding</th>\n",
    "\t\t<th>Number of Parameters</th>\n",
    "\t\t<th>Forward Computation</th>\n",
    "\t</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\t<tr>\n",
    "\t\t<td>3* 227 * 227</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv1 + Relu</td>\n",
    "\t\t<td>11 * 11</td>\n",
    "\t\t<td>96</td>\n",
    "\t\t<td>4</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>(11*11*3 + 1) * 96=34944</td>\n",
    "\t\t<td>(11*11*3 + 1) * 96 * 55 * 55=105705600</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>96 * 55 * 55</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>96 * 27 * 27</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv2 + Relu</td>\n",
    "\t\t<td>5 * 5</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256=614656</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256 * 27 * 27=448084224</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 27 * 27</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv3 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384=885120</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384 * 13 * 13=149585280</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>384 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv4 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384=1327488</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384 * 13 * 13=224345472</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>384 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv5 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256=884992</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256 * 13 * 13=149563648</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 6 * 6</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC6 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n",
    "\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>4096</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC7 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 4096=16777216</td>\n",
    "\t\t<td>4096 * 4096=16777216</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>4096</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC8 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 1000=4096000</td>\n",
    "\t\t<td>4096 * 1000=4096000</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>1000 classes</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Overall</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>62369152=62.3 million</td>\n",
    "\t\t<td>1135906176=1.1 billion</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv VS FC</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>Conv:3.7million (6%) , FC: 58.6 million  (94% )</td>\n",
    "\t\t<td>Conv: 1.08 billion (95%) , FC: 58.6 million (5%)</td>\n",
    "\t</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does AlexNet achieve better results?\n",
    "\n",
    "1. **Relu activation function is used.**\n",
    "\n",
    "Relu function: f (x) = max (0, x)\n",
    "\n",
    "![alex1](img/alex512.png)\n",
    "\n",
    "ReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based networks. The following figure shows the number of iterations for a four-layer convolutional network based on CIFAR-10 that reached 25% training error in tanh and ReLU:\n",
    "\n",
    "![alex1](img/alex612.png)\n",
    "\n",
    "2. **Standardization ( Local Response Normalization )**\n",
    "\n",
    "After using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the tanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady proposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\", which talks about the effect of active neurons on its surrounding neurons.\n",
    "\n",
    "![alex1](img/alex3.jpg)\n",
    "\n",
    "\n",
    "3. **Dropout**\n",
    "\n",
    "Dropout is also a concept often said, which can effectively prevent overfitting of neural networks. Compared to the general linear model, a regular method is used to prevent the model from overfitting. In the neural network, Dropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons, randomly delete some neurons with a defined probability, while keeping the individuals of the input layer and output layer neurons unchanged, and then update the parameters according to the learning method of the neural network. In the next iteration, rerandom Remove some neurons until the end of training.\n",
    "\n",
    "\n",
    "![alex1](img/alex4.jpg)\n",
    "\n",
    "\n",
    "4. **Enhanced Data ( Data Augmentation )**\n",
    "\n",
    "\n",
    "\n",
    "**In deep learning, when the amount of data is not large enough, there are generally 4 solutions:**\n",
    "\n",
    ">  Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data from existing data by means of translation, flipping, noise\n",
    "\n",
    ">  Regularization——The relatively small amount of data will cause the model to overfit, making the training error small and the test error particularly large. By adding a regular term after the Loss Function , the overfitting can be suppressed. The disadvantage is that a need is introduced Manually adjusted hyper-parameter.\n",
    "\n",
    ">  Dropout- also a regularization method. But different from the above, it is achieved by randomly setting the output of some neurons to zero\n",
    "\n",
    ">  Unsupervised Pre-training- use Auto-Encoder or RBM's convolution form to do unsupervised pre-training layer by layer, and finally add a classification layer to do supervised Fine-Tuning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 54, 54, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 27, 27, 96)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 27, 27, 96)        384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 17, 17, 256)       2973952   \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 17, 17, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 8, 8, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 8, 8, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 6, 6, 384)         885120    \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 6, 6, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 6, 6, 384)         1536      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 4, 4, 384)         1327488   \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 4, 4, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 4, 4, 384)         1536      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 2, 2, 256)         884992    \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 1, 1, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 1, 1, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 4096)              1052672   \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 4096)              16384     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 4096)              16384     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1000)              4097000   \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 1000)              4000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 17)                17017     \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 17)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28096769 (107.18 MB)\n",
      "Trainable params: 28075633 (107.10 MB)\n",
      "Non-trainable params: 21136 (82.56 KB)\n",
      "_________________________________________________________________\n",
      "Train on 979 samples, validate on 245 samples\n",
      "Epoch 1/30\n",
      "979/979 [==============================] - 40s 41ms/sample - loss: 4.1171 - acc: 0.0613 - val_loss: 42.5922 - val_acc: 0.0571\n",
      "Epoch 2/30\n",
      "979/979 [==============================] - 42s 42ms/sample - loss: 3.6106 - acc: 0.0725 - val_loss: 38.9954 - val_acc: 0.0735\n",
      "Epoch 3/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 3.3213 - acc: 0.0930 - val_loss: 13.6434 - val_acc: 0.0653\n",
      "Epoch 4/30\n",
      "979/979 [==============================] - 44s 45ms/sample - loss: 3.2483 - acc: 0.0817 - val_loss: 4.4150 - val_acc: 0.0653\n",
      "Epoch 5/30\n",
      "979/979 [==============================] - 42s 43ms/sample - loss: 3.2308 - acc: 0.1021 - val_loss: 3.2364 - val_acc: 0.0571\n",
      "Epoch 6/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 3.1464 - acc: 0.1052 - val_loss: 3.7298 - val_acc: 0.0612\n",
      "Epoch 7/30\n",
      "979/979 [==============================] - 41s 42ms/sample - loss: 3.0175 - acc: 0.1226 - val_loss: 3.0527 - val_acc: 0.0490\n",
      "Epoch 8/30\n",
      "979/979 [==============================] - 41s 42ms/sample - loss: 2.9517 - acc: 0.1277 - val_loss: 3.0553 - val_acc: 0.0531\n",
      "Epoch 9/30\n",
      "979/979 [==============================] - 41s 42ms/sample - loss: 2.9003 - acc: 0.1553 - val_loss: 3.1437 - val_acc: 0.0531\n",
      "Epoch 10/30\n",
      "979/979 [==============================] - 42s 42ms/sample - loss: 2.8703 - acc: 0.1563 - val_loss: 3.0949 - val_acc: 0.0490\n",
      "Epoch 11/30\n",
      "979/979 [==============================] - 44s 45ms/sample - loss: 2.8193 - acc: 0.1716 - val_loss: 3.1178 - val_acc: 0.0612\n",
      "Epoch 12/30\n",
      "979/979 [==============================] - 42s 43ms/sample - loss: 2.8221 - acc: 0.1818 - val_loss: 3.0781 - val_acc: 0.0816\n",
      "Epoch 13/30\n",
      "979/979 [==============================] - 41s 42ms/sample - loss: 2.6609 - acc: 0.2074 - val_loss: 3.2143 - val_acc: 0.0735\n",
      "Epoch 14/30\n",
      "979/979 [==============================] - 41s 42ms/sample - loss: 2.6173 - acc: 0.2257 - val_loss: 3.2462 - val_acc: 0.0571\n",
      "Epoch 15/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 2.4276 - acc: 0.2819 - val_loss: 3.5339 - val_acc: 0.0571\n",
      "Epoch 16/30\n",
      "979/979 [==============================] - 42s 43ms/sample - loss: 2.4070 - acc: 0.2819 - val_loss: 3.4800 - val_acc: 0.0735\n",
      "Epoch 17/30\n",
      "979/979 [==============================] - 42s 43ms/sample - loss: 2.3396 - acc: 0.2829 - val_loss: 3.6424 - val_acc: 0.0653\n",
      "Epoch 18/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 2.2045 - acc: 0.3432 - val_loss: 3.8168 - val_acc: 0.0816\n",
      "Epoch 19/30\n",
      "979/979 [==============================] - 42s 43ms/sample - loss: 2.0826 - acc: 0.3647 - val_loss: 4.0888 - val_acc: 0.0612\n",
      "Epoch 20/30\n",
      "979/979 [==============================] - 44s 45ms/sample - loss: 2.0183 - acc: 0.3739 - val_loss: 4.0445 - val_acc: 0.0531\n",
      "Epoch 21/30\n",
      "979/979 [==============================] - 43s 43ms/sample - loss: 1.8608 - acc: 0.4351 - val_loss: 4.6351 - val_acc: 0.0571\n",
      "Epoch 22/30\n",
      "979/979 [==============================] - 40s 41ms/sample - loss: 1.7850 - acc: 0.4566 - val_loss: 5.0764 - val_acc: 0.0531\n",
      "Epoch 23/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 1.5704 - acc: 0.5189 - val_loss: 5.2925 - val_acc: 0.0490\n",
      "Epoch 24/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 1.3057 - acc: 0.6047 - val_loss: 4.9836 - val_acc: 0.0857\n",
      "Epoch 25/30\n",
      "979/979 [==============================] - 42s 43ms/sample - loss: 1.2187 - acc: 0.6282 - val_loss: 5.3714 - val_acc: 0.0735\n",
      "Epoch 26/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 1.1164 - acc: 0.6435 - val_loss: 5.6764 - val_acc: 0.0735\n",
      "Epoch 27/30\n",
      "979/979 [==============================] - 44s 45ms/sample - loss: 1.0469 - acc: 0.6629 - val_loss: 5.8977 - val_acc: 0.0449\n",
      "Epoch 28/30\n",
      "979/979 [==============================] - 43s 44ms/sample - loss: 1.0313 - acc: 0.6782 - val_loss: 6.2036 - val_acc: 0.0571\n",
      "Epoch 29/30\n",
      "979/979 [==============================] - 44s 45ms/sample - loss: 1.0179 - acc: 0.6762 - val_loss: 7.0302 - val_acc: 0.0694\n",
      "Epoch 30/30\n",
      "979/979 [==============================] - 44s 44ms/sample - loss: 0.8904 - acc: 0.7160 - val_loss: 5.9704 - val_acc: 0.0694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2310d69ee00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "import tarfile\n",
    "import os\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# (2) Get Data\n",
    "def load_oxflower17_data(img_size=(224, 224), one_hot=True):\n",
    "    # Download the dataset\n",
    "    url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz\"\n",
    "    path = get_file(\"17flowers.tgz\", url)\n",
    "\n",
    "    # Extract the dataset\n",
    "    with tarfile.open(path, 'r') as file:\n",
    "        file.extractall(path=os.path.dirname(path))\n",
    "\n",
    "    # Load images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    base_path = os.path.join(os.path.dirname(path), \"jpg\")\n",
    "    for i, filename in enumerate(sorted(os.listdir(base_path))):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img = load_img(os.path.join(base_path, filename), target_size=img_size)\n",
    "            images.append(img_to_array(img))\n",
    "            labels.append(i % 17)  # Wrap around the label values\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    # Normalize the images\n",
    "    images /= 255.0\n",
    "\n",
    "    # One-hot encode the labels if required\n",
    "    if one_hot:\n",
    "        labels = tf.keras.utils.to_categorical(labels, 17)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load the dataset\n",
    "x, y = load_oxflower17_data()\n",
    "\n",
    "# Use only 10% of the data\n",
    "data_size = int(0.9 * len(x))\n",
    "x = x[:data_size]\n",
    "y = y[:data_size]\n",
    "\n",
    "# (3) Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# (4) Compile \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# (5) Train\n",
    "model.fit(x, y, batch_size=64, epochs=30, verbose=1, \\\n",
    "validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
